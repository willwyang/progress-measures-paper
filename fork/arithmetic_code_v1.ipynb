{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import einops\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "checkpoint_every = 1000\n",
    "checkpoint_path = \"/Users/williamyang/Documents/local_projects/arithmetic/checkpoints/\"\n",
    "checkpoint_epochs, model_checkpoints = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 113\n",
    "\n",
    "a_vector = einops.repeat(torch.arange(P), 'i -> (i j)', j=P)\n",
    "b_vector = einops.repeat(torch.arange(P), 'j -> (i j)', i=P)\n",
    "equals_vector = einops.repeat(torch.tensor(P), '-> (i j)', i=P, j=P)\n",
    "\n",
    "X = torch.stack([a_vector, b_vector, equals_vector], dim=1)\n",
    "Y = (X[:, 0] + X[:, 1]) % P\n",
    "\n",
    "X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "train_size = int(0.3 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 128,\n",
    "    d_head = 32,\n",
    "    d_mlp = 512,\n",
    "    act_fn = 'relu',\n",
    "    normalization_type = None,\n",
    "    d_vocab = P+1,\n",
    "    d_vocab_out = P,\n",
    "    n_ctx = 3,\n",
    "    init_weights = True\n",
    ")\n",
    "\n",
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'b_' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1, betas=(0.9, 0.98))\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    if len(logits.shape) == 3:\n",
    "        logits = logits[:, -1].to(device)\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25000\n",
    "\n",
    "train_losses, test_losses = np.zeros(num_epochs+1), np.zeros(num_epochs+1)\n",
    "activations = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_logits, cache = model.run_with_cache(train_dataset[:][0])\n",
    "    train_loss = loss_fn(train_logits, train_dataset[:][1])\n",
    "    train_loss.backward()\n",
    "    train_losses[epoch] = train_loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        activations.append(cache)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_dataset[:][0])\n",
    "        test_loss = loss_fn(test_logits, test_dataset[:][1])\n",
    "        test_losses[epoch] = test_loss.item()\n",
    "\n",
    "    if (epoch+1)%checkpoint_every:\n",
    "        checkpoint_epochs.append(epoch)\n",
    "        model_checkpoints.append(copy.deepcopy(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grokking_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
